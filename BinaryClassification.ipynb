{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical testing in binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neccessary stuff\n",
    "1. Train two or more models.\n",
    "1. Evaluate trained models on one or more test datasets (it's a little bit tricky when you want to make several datasets from one).\n",
    "1. Ask a question you want to answer about this models.\n",
    "1. Choose statistical test equivalent to your question.\n",
    "1. Measure neccessary things for this test.\n",
    "1. Assume some significance level, e.g. $\\alpha = 0.05$. \n",
    "1. Calculate p-value for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two models, one data set, what can I do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to store trained models\n",
    "models_dir = './models/'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.mkdir(models_dir)\n",
    "\n",
    "# The path to the directory where the original dataset was uncompressed\n",
    "original_dataset_dir = './Dogs-vs-Cats-1'\n",
    "original_cat_dir = './Dogs-vs-Cats-1/Cat'\n",
    "original_dog_dir = './Dogs-vs-Cats-1/Dog'\n",
    "\n",
    "# The directory where we will store our smaller dataset\n",
    "base_dir = './Dogs-vs-Cats-1/working'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training, validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "# Directory with training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "if not os.path.exists(train_cats_dir):\n",
    "    os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "if not os.path.exists(train_dogs_dir):\n",
    "    os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "if not os.path.exists(validation_cats_dir):\n",
    "    os.mkdir(validation_cats_dir)\n",
    "    \n",
    "# Directory with validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "if not os.path.exists(validation_dogs_dir):\n",
    "    os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with test cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "if not os.path.exists(test_cats_dir):\n",
    "    os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with test dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "if not os.path.exists(test_dogs_dir):\n",
    "    os.mkdir(test_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['{}.jpg'.format(i) for i in range(1001)]\n",
    "for fname in fnames:\n",
    "    if fname == '666.jpg':\n",
    "        continue\n",
    "    src = os.path.join(original_cat_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['{}.jpg'.format(i) for i in range(1001, 1501)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_cat_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['{}.jpg'.format(i) for i in range(1501, 2001)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_cat_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dog_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dog_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dog_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More test sets from first dataset\n",
    "test_dirs = []\n",
    "for i in range(19):\n",
    "    test_dirs.append(os.path.join(base_dir, f'test_{i}'))\n",
    "    if not os.path.exists(test_dirs[i]):\n",
    "        os.mkdir(test_dirs[i])\n",
    "\n",
    "    temp_test_cats_dir = os.path.join(test_dirs[i], 'cats')\n",
    "    if not os.path.exists(temp_test_cats_dir):\n",
    "        os.mkdir(temp_test_cats_dir)\n",
    "\n",
    "    temp_test_dogs_dir = os.path.join(test_dirs[i], 'dogs')\n",
    "    if not os.path.exists(temp_test_dogs_dir):\n",
    "        os.mkdir(temp_test_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_dirs)):\n",
    "    temp_test_cats_dir = os.path.join(test_dirs[i], 'cats')\n",
    "    fnames = ['{}.jpg'.format(i) for i in range(2001+i*500, 2501+i*500)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_cat_dir, fname)\n",
    "        dst = os.path.join(temp_test_cats_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    temp_test_dogs_dir = os.path.join(test_dirs[i], 'dogs')\n",
    "    fnames = ['{}.jpg'.format(i) for i in range(2000+i*500, 2500+i*500)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(original_dog_dir, fname)\n",
    "        dst = os.path.join(temp_test_dogs_dir, fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_rows = 150\n",
    "img_cols = 150\n",
    "\n",
    "# data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='binary')\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_rows = 150\n",
    "img_cols = 150\n",
    "\n",
    "# Additional test generators\n",
    "test_generators = []\n",
    "for dir in test_dirs:\n",
    "    test_generators.append(test_datagen.flow_from_directory(\n",
    "        dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_rows, img_cols, 3)))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tosiek/Documents/VI_semestr/GiGSN/statistical-significance-ml/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718034791.795290   13587 service.cc:145] XLA service 0x72a8cc00ac70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1718034791.795315   13587 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 Ti, Compute Capability 7.5\n",
      "2024-06-10 17:53:11.856243: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-10 17:53:12.094004: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/63\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:27\u001b[0m 9s/step - acc: 0.6562 - loss: 0.6909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1718034798.751011   13587 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 250ms/step - acc: 0.5094 - loss: 0.6946 - val_acc: 0.5030 - val_loss: 0.6922\n",
      "Epoch 2/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 133ms/step - acc: 0.5372 - loss: 0.6908 - val_acc: 0.5030 - val_loss: 0.6884\n",
      "Epoch 3/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - acc: 0.5322 - loss: 0.6904 - val_acc: 0.5700 - val_loss: 0.6800\n",
      "Epoch 4/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - acc: 0.5822 - loss: 0.6827 - val_acc: 0.6310 - val_loss: 0.6705\n",
      "Epoch 5/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 135ms/step - acc: 0.5868 - loss: 0.6760 - val_acc: 0.6060 - val_loss: 0.6573\n",
      "Epoch 6/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 146ms/step - acc: 0.5991 - loss: 0.6635 - val_acc: 0.6420 - val_loss: 0.6270\n",
      "Epoch 7/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 135ms/step - acc: 0.6068 - loss: 0.6486 - val_acc: 0.5040 - val_loss: 0.8120\n",
      "Epoch 8/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 139ms/step - acc: 0.6177 - loss: 0.6434 - val_acc: 0.6600 - val_loss: 0.6114\n",
      "Epoch 9/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 138ms/step - acc: 0.6589 - loss: 0.6204 - val_acc: 0.6960 - val_loss: 0.5928\n",
      "Epoch 10/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - acc: 0.6606 - loss: 0.6160 - val_acc: 0.7030 - val_loss: 0.5763\n",
      "Epoch 11/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.6503 - loss: 0.6181 - val_acc: 0.6780 - val_loss: 0.6060\n",
      "Epoch 12/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - acc: 0.6678 - loss: 0.6027 - val_acc: 0.6850 - val_loss: 0.5776\n",
      "Epoch 13/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - acc: 0.6576 - loss: 0.6108 - val_acc: 0.7160 - val_loss: 0.5576\n",
      "Epoch 14/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.6761 - loss: 0.5940 - val_acc: 0.7000 - val_loss: 0.5903\n",
      "Epoch 15/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 151ms/step - acc: 0.7072 - loss: 0.5811 - val_acc: 0.7280 - val_loss: 0.5677\n",
      "Epoch 16/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 147ms/step - acc: 0.7027 - loss: 0.5735 - val_acc: 0.7120 - val_loss: 0.5374\n",
      "Epoch 17/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.6854 - loss: 0.5926 - val_acc: 0.6790 - val_loss: 0.5758\n",
      "Epoch 18/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 138ms/step - acc: 0.6934 - loss: 0.5829 - val_acc: 0.6290 - val_loss: 0.6130\n",
      "Epoch 19/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.6922 - loss: 0.5802 - val_acc: 0.6260 - val_loss: 0.6537\n",
      "Epoch 20/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 139ms/step - acc: 0.7030 - loss: 0.5804 - val_acc: 0.6710 - val_loss: 0.6089\n",
      "Epoch 21/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 138ms/step - acc: 0.7085 - loss: 0.5529 - val_acc: 0.7440 - val_loss: 0.5221\n",
      "Epoch 22/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 139ms/step - acc: 0.7129 - loss: 0.5547 - val_acc: 0.6090 - val_loss: 0.7826\n",
      "Epoch 23/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 139ms/step - acc: 0.7212 - loss: 0.5436 - val_acc: 0.7180 - val_loss: 0.5377\n",
      "Epoch 24/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.7576 - loss: 0.5174 - val_acc: 0.7480 - val_loss: 0.4986\n",
      "Epoch 25/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - acc: 0.6992 - loss: 0.5585 - val_acc: 0.7470 - val_loss: 0.5071\n",
      "Epoch 26/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.7432 - loss: 0.5294 - val_acc: 0.7440 - val_loss: 0.5120\n",
      "Epoch 27/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - acc: 0.7296 - loss: 0.5462 - val_acc: 0.7480 - val_loss: 0.5064\n",
      "Epoch 28/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.7583 - loss: 0.5073 - val_acc: 0.7490 - val_loss: 0.5001\n",
      "Epoch 29/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - acc: 0.7558 - loss: 0.5232 - val_acc: 0.7360 - val_loss: 0.5281\n",
      "Epoch 30/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.7376 - loss: 0.5186 - val_acc: 0.7560 - val_loss: 0.4956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x72aa1d343a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=3e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model1.fit(\n",
    "      train_generator,\n",
    "      batch_size=batch_size,\n",
    "      epochs=30,\n",
    "      validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model1.save(models_dir+\"binary_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_rows, img_cols, 3)))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - acc: 0.5131 - loss: 0.6934 - val_acc: 0.5090 - val_loss: 0.6891\n",
      "Epoch 2/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 147ms/step - acc: 0.5520 - loss: 0.6891 - val_acc: 0.5830 - val_loss: 0.6750\n",
      "Epoch 3/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 139ms/step - acc: 0.5871 - loss: 0.6749 - val_acc: 0.6200 - val_loss: 0.6494\n",
      "Epoch 4/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.6323 - loss: 0.6492 - val_acc: 0.6300 - val_loss: 0.6321\n",
      "Epoch 5/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - acc: 0.6410 - loss: 0.6289 - val_acc: 0.6580 - val_loss: 0.6235\n",
      "Epoch 6/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.6644 - loss: 0.6230 - val_acc: 0.6760 - val_loss: 0.6075\n",
      "Epoch 7/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - acc: 0.6680 - loss: 0.6172 - val_acc: 0.6550 - val_loss: 0.6143\n",
      "Epoch 8/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - acc: 0.6753 - loss: 0.6252 - val_acc: 0.6600 - val_loss: 0.5965\n",
      "Epoch 9/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - acc: 0.6666 - loss: 0.6088 - val_acc: 0.6980 - val_loss: 0.5761\n",
      "Epoch 10/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 146ms/step - acc: 0.6936 - loss: 0.5923 - val_acc: 0.7040 - val_loss: 0.5875\n",
      "Epoch 11/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 155ms/step - acc: 0.6960 - loss: 0.5888 - val_acc: 0.7050 - val_loss: 0.5898\n",
      "Epoch 12/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - acc: 0.6836 - loss: 0.5843 - val_acc: 0.7230 - val_loss: 0.5547\n",
      "Epoch 13/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - acc: 0.7277 - loss: 0.5553 - val_acc: 0.7040 - val_loss: 0.5695\n",
      "Epoch 14/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.7088 - loss: 0.5727 - val_acc: 0.6950 - val_loss: 0.5694\n",
      "Epoch 15/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.7303 - loss: 0.5419 - val_acc: 0.7440 - val_loss: 0.5175\n",
      "Epoch 16/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.7251 - loss: 0.5455 - val_acc: 0.7370 - val_loss: 0.5286\n",
      "Epoch 17/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 146ms/step - acc: 0.7340 - loss: 0.5269 - val_acc: 0.7230 - val_loss: 0.5446\n",
      "Epoch 18/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - acc: 0.7016 - loss: 0.5731 - val_acc: 0.7430 - val_loss: 0.5144\n",
      "Epoch 19/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 148ms/step - acc: 0.7544 - loss: 0.5193 - val_acc: 0.7520 - val_loss: 0.5041\n",
      "Epoch 20/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 146ms/step - acc: 0.7397 - loss: 0.5272 - val_acc: 0.6690 - val_loss: 0.6128\n",
      "Epoch 21/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - acc: 0.7233 - loss: 0.5541 - val_acc: 0.7610 - val_loss: 0.4947\n",
      "Epoch 22/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - acc: 0.7384 - loss: 0.5195 - val_acc: 0.7510 - val_loss: 0.5243\n",
      "Epoch 23/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - acc: 0.7441 - loss: 0.5394 - val_acc: 0.7660 - val_loss: 0.4748\n",
      "Epoch 24/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - acc: 0.7641 - loss: 0.4854 - val_acc: 0.7730 - val_loss: 0.4714\n",
      "Epoch 25/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - acc: 0.7791 - loss: 0.4844 - val_acc: 0.7530 - val_loss: 0.4882\n",
      "Epoch 26/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.7663 - loss: 0.4870 - val_acc: 0.7460 - val_loss: 0.5087\n",
      "Epoch 27/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.7778 - loss: 0.4851 - val_acc: 0.7780 - val_loss: 0.4680\n",
      "Epoch 28/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - acc: 0.7877 - loss: 0.4715 - val_acc: 0.7520 - val_loss: 0.4902\n",
      "Epoch 29/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - acc: 0.8049 - loss: 0.4360 - val_acc: 0.7690 - val_loss: 0.4781\n",
      "Epoch 30/30\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - acc: 0.7763 - loss: 0.4524 - val_acc: 0.7710 - val_loss: 0.4506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x72aa1c3633d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=3e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model2.fit(\n",
    "      train_generator,\n",
    "      batch_size=batch_size,\n",
    "      epochs=30,\n",
    "      validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model2.save(models_dir+\"binary_model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to know if one model is better than other?\n",
    "Perform McNemar's test! It's simple statistic based on *False Negatives* clasified only one of the two models. The statistic of this test is calucated by equation below:\n",
    "$$\\chi^2 = \\frac{(|b-c|-1)^2}{b+c}$$\n",
    "* b - *False Negatives* clasified by first model, but not by the second,\n",
    "* c - *False Negatives* clasified by second model, but not the first.\n",
    "\n",
    "This test follows chi-squared distribution with one degree of freedom when $b+c\\geq20$, and binomial distribution in the other case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model1 = load_model(models_dir+\"binary_model1.h5\")\n",
    "model2 = load_model(models_dir+\"binary_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict \n",
    "y_true = test_generator.labels\n",
    "size = len(y_true)\n",
    "y_pred1 = np.array([model1.predict(test_generator) > 0.5], dtype=int).reshape((size,))\n",
    "y_pred2 = np.array([model2.predict(test_generator) > 0.5], dtype=int).reshape((size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives classified by first model, but not by the second: 66\n",
      "False negatives classified by second model, but not by the first: 19\n"
     ]
    }
   ],
   "source": [
    "# Calculate b and c\n",
    "b = 0\n",
    "c = 0\n",
    "for i in range(size):\n",
    "    if y_true[i] == 1:\n",
    "        if y_pred1[i] == 0 and y_pred2[i] == 1:\n",
    "            b += 1\n",
    "        elif y_pred1[i] == 1 and y_pred2[i] == 0:\n",
    "            c += 1 \n",
    "\n",
    "print(\"False negatives classified by first model, but not by the second:\", b)\n",
    "print(\"False negatives classified by second model, but not by the first:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2113390294598264e-06\n"
     ]
    }
   ],
   "source": [
    "# Calulate test statistic\n",
    "chi2 = (abs(b-c) - 1)**2/(b+c)\n",
    "p = 2 * min(scipy.stats.chi2.sf(chi2, 1), scipy.stats.chi2.cdf(chi2, 1))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between ROC curves\n",
    "There is also a posibility to check if there is a significant difference between ROC curves, perfolming DeLong's test. This test is a little bit more demanding computionally, because we need to estimate some values. Test statistics is obtained by equation below.\n",
    "$$z_D=\\frac{\\hat\\theta_1-\\hat\\theta_2}{\\sqrt{Var(\\hat\\theta_1)+Var(\\hat\\theta_2)-2Cov(\\hat\\theta_1,\\hat\\theta_2)}}$$\n",
    "\n",
    "In the equation above, $\\hat\\theta_k$ is an estimated *Area under ROC Curve* of k-th model. We can obtain this value by formula given below.\n",
    "$$\\hat\\theta=\\frac{1}{mn}\\sum_{i=1}^m\\sum_{j=1}^n\\Psi(Y_{i1},Y_{j0}), \\Psi(Y_{i1},Y_{j0})=\\begin{cases}\n",
    "      1 & Y_{i1} > Y_{j0}\\\\\n",
    "      \\frac{1}{2} & Y_{i1} = Y_{j0}\\\\\n",
    "      0 & Y_{i1} < Y_{j0}\n",
    "    \\end{cases}$$ \n",
    "Where $Y_{i1}$ is numeric prediction of i-th truly positive instance, and $Y_{j0}$ is numeric prediction of j-th truly negative instance, and $m$ and $n$ are, correspondingly, numbers of this instances. \n",
    "\n",
    "Calculating estimates of variance and coveriance is a little bit more tricky. First we need to caluclate *structual components*. Structural components are defined separetly for truly positive and negative instances. Structural element of r-th model for i-th truly positive instance is calculated by the formula given below. \n",
    "$$V^r_{10}(Y_{i1})=\\frac{1}{n}\\sum_{j=1}^n\\Psi(Y_{i1}, Y_{j0})$$ \n",
    "\n",
    "Similarly we define structural element for j-th truly negative instance.\n",
    "$$V^r_{01}(Y_{j0})=\\frac{1}{m}\\sum_{i=1}^m\\Psi(Y_{i1}, Y_{j0})$$\n",
    "\n",
    "Having structural elements defined we can compute matrices $S_{10}$ and $S_{01}$. Value in r-th row and s-th column is given by the following equations.\n",
    "$$S^{(r,s)}_{10}=\\frac{1}{m-1}\\sum^m_{i=1}[V^r_{10}(Y_{i1})-\\hat\\theta_r][V^s_{10}(Y_{i1})-\\hat\\theta_s]$$\n",
    "$$S^{(r,s)}_{01}=\\frac{1}{n-1}\\sum^n_{j=1}[V^r_{01}(Y_{j0})-\\hat\\theta_r][V^s_{01}(Y_{j0})-\\hat\\theta_s]$$\n",
    "\n",
    "Once we calculate matricies $S_{10}$ and $S_{01}$, we can calculate matrix S from which we read values for estimated values of variance and covariance. Formula for this matrix is presented below.\n",
    "$$S=\\frac{1}{m}S_{10}+\\frac{1}{n}S_{01}=\\begin{Bmatrix}\n",
    "Var(\\hat\\theta_1) & Cov(\\hat\\theta_1,\\hat\\theta_2) \\\\\n",
    "Cov(\\hat\\theta_2,\\hat\\theta_1) & Var(\\hat\\theta_2)\n",
    "\\end{Bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(y1, y0):\n",
    "    if y1 > y0:\n",
    "        return 1\n",
    "    if y1 == y0:\n",
    "        return 0.5\n",
    "    return 0\n",
    "\n",
    "def v10(y1, y0):\n",
    "    n = len(y0)\n",
    "    sum = 0\n",
    "    for j in y0:\n",
    "        sum += psi(y1, j)\n",
    "    return sum / n\n",
    "\n",
    "def v01(y0, y1):\n",
    "    m = len(y1)\n",
    "    sum = 0\n",
    "    for i in y1:\n",
    "        sum += psi(i, y0)\n",
    "    return sum / m\n",
    "\n",
    "def s10(y1, y0, thetas):\n",
    "    m = len(y1[0])\n",
    "    s_ = np.zeros((2,2))\n",
    "    for r in range(2):\n",
    "        for s in range(2):\n",
    "            for i in range(m):\n",
    "               s_[r, s] += (v10(y1[r][i], y0[r])-thetas[r])*(v10(y1[s][i], y0[s])-thetas[s])\n",
    "            s_[r, s] /= (m-1)\n",
    "    return s_\n",
    "\n",
    "def s01(y1, y0, thetas):\n",
    "    n = len(y0[0])\n",
    "    s_ = np.zeros((2,2))\n",
    "    for r in range(2):\n",
    "        for s in range(2):\n",
    "            for j in range(n):\n",
    "               s_[r, s] += (v01(y0[r][j], y1[r])-thetas[r])*(v01(y0[s][j], y1[s])-thetas[s])\n",
    "            s_[r, s] /= (n-1)\n",
    "    return s_\n",
    "\n",
    "def theta(y1, y0):\n",
    "    m = len(y1)\n",
    "    n = len(y0)\n",
    "    sum = 0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            sum += psi(y1[i], y0[j])\n",
    "    return sum/(m*n)\n",
    "\n",
    "def delong_test(y_true, y_pred1, y_pred2):\n",
    "    y1 = [[], []]\n",
    "    y0 = [[], []]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 0:\n",
    "            y0[0].append(y_pred1[i])\n",
    "            y0[1].append(y_pred2[i])\n",
    "        else:\n",
    "            y1[0].append(y_pred1[i])\n",
    "            y1[1].append(y_pred2[i])\n",
    "    thetas = [theta(y1[0], y0[0]), theta(y1[1], y0[1])]\n",
    "    m = len(y1[0])\n",
    "    n = len(y0[0])\n",
    "    s = s10(y1, y0, thetas)/m + s01(y1, y0, thetas)/n\n",
    "    var1 = s[0, 0]\n",
    "    var2 = s[1, 1]\n",
    "    cov = s[0, 1]\n",
    "    z = (thetas[0]-thetas[1])/(var1 + var2 - 2*cov)**0.5\n",
    "    p = 2*min(scipy.stats.norm.sf(z), scipy.stats.norm.cdf(z))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model1 = load_model(models_dir+\"binary_model1.h5\")\n",
    "model2 = load_model(models_dir+\"binary_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict \n",
    "y_true = test_generator.labels\n",
    "y_pred1 = model1.predict(test_generator)\n",
    "y_pred2 = model2.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8305778345316294e-05"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delong_test(y_true, y_pred1, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we have two models and more data?\n",
    "At this point we have two possible questions we can answer. First:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a significant difference in this models.\n",
    "If we want to answer this question, we can use Wilcoxon test!\n",
    "First we need to evaluate choosen metric for all test sets. After that we calculate differences between metrics, for corresponding test sets. After that, we order this differences by absoulte value, and then calculate two values, using formula below, knowing that $d_i$ is difference between metrics obtained on i-th set, and $rank(|d_i|)$ is place of that difference in our series. \n",
    "$$R^+=\\sum_{d_i>0}rank(|d_i|)$$\n",
    "$$R^-=\\sum_{d_i<0}rank(|d_i|)$$\n",
    "\n",
    "As test statistic we can take value: \n",
    "$$T=min\\{R^+, R^-\\}$$\n",
    "Or, if we have large number (n) of test cases, we can use statistic $z$, which follows normal distribution:\n",
    "$$z=\\frac{T-\\frac{n(n+1)}{4}}{\\sqrt(\\frac{n(n+1)(2n+1)}{24})}$$ \n",
    "\n",
    "To compute this statistics, we don't need to do everything manually this time, we can use method from scipy.stats: wicloxon. By default this method uses statistic $T$, when number of test cases is less or equal to 50, but we can choose if we want to use statistic $T$ or $z$, by passing argument *method* with value - respectively - *exact* and *approx*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model1 = load_model(models_dir+\"binary_model1.h5\")\n",
    "model2 = load_model(models_dir+\"binary_model2.h5\")\n",
    "\n",
    "model1.compile(optimizer=model1.optimizer,\n",
    "                        loss=model1.loss,\n",
    "                        metrics=['acc', 'auc', 'precision', 'recall'])\n",
    "model2.compile(optimizer=model2.optimizer,\n",
    "                        loss=model2.loss,\n",
    "                        metrics=['acc', 'auc', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tosiek/Documents/VI_semestr/GiGSN/statistical-significance-ml/.venv/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model accuracies: [[0.7699999809265137, 0.847681999206543, 0.8139534592628479, 0.699999988079071], [0.746999979019165, 0.8079699277877808, 0.7905882596969604, 0.671999990940094], [0.7400000095367432, 0.8189380168914795, 0.7898550629615784, 0.6539999842643738], [0.7490000128746033, 0.8129520416259766, 0.792941153049469, 0.6740000247955322], [0.7540000081062317, 0.8391139507293701, 0.8038277626037598, 0.671999990940094], [0.75, 0.8264439702033997, 0.7765486836433411, 0.7020000219345093], [0.7289999723434448, 0.8130620121955872, 0.7584649920463562, 0.671999990940094], [0.7390000224113464, 0.8294360637664795, 0.7697516679763794, 0.6819999814033508], [0.7630000114440918, 0.8403159976005554, 0.8036951422691345, 0.6959999799728394], [0.7540000081062317, 0.8262599110603333, 0.7834821343421936, 0.7020000219345093], [0.777999997138977, 0.8302860856056213, 0.8130630850791931, 0.722000002861023], [0.7139999866485596, 0.8054440021514893, 0.7661691308021545, 0.6159999966621399], [0.746999979019165, 0.8320460319519043, 0.7787810564041138, 0.6899999976158142], [0.7710000276565552, 0.838528037071228, 0.8044943809509277, 0.7160000205039978], [0.7289999723434448, 0.8126120567321777, 0.7632184028625488, 0.6639999747276306], [0.7630000114440918, 0.842456042766571, 0.8094117641448975, 0.6880000233650208], [0.7509999871253967, 0.8319480419158936, 0.7832956910133362, 0.6940000057220459], [0.7360000014305115, 0.8268299698829651, 0.7892156839370728, 0.6439999938011169], [0.7549999952316284, 0.8455559611320496, 0.8132678270339966, 0.6620000004768372]]\n",
      "Second model accuracies: [[0.781000018119812, 0.8760700225830078, 0.7707129120826721, 0.800000011920929], [0.7940000295639038, 0.8659219145774841, 0.7963709831237793, 0.7900000214576721], [0.7699999809265137, 0.8522260189056396, 0.7710843086242676, 0.7680000066757202], [0.7739999890327454, 0.8561200499534607, 0.7751004099845886, 0.7720000147819519], [0.8050000071525574, 0.8909260630607605, 0.8157349824905396, 0.7879999876022339], [0.7850000262260437, 0.8711420893669128, 0.7810651063919067, 0.7919999957084656], [0.7860000133514404, 0.8492399454116821, 0.7749999761581421, 0.8059999942779541], [0.7950000166893005, 0.8625040054321289, 0.7944111824035645, 0.7960000038146973], [0.8069999814033508, 0.8802520632743835, 0.803960382938385, 0.8119999766349792], [0.7730000019073486, 0.8644800186157227, 0.7640231847763062, 0.7900000214576721], [0.7950000166893005, 0.8696080446243286, 0.7842003703117371, 0.8140000104904175], [0.7680000066757202, 0.8468400239944458, 0.7887930870056152, 0.7319999933242798], [0.7730000019073486, 0.8657580018043518, 0.763005793094635, 0.7919999957084656], [0.7860000133514404, 0.8640180826187134, 0.7860000133514404, 0.7860000133514404], [0.781000018119812, 0.8658220171928406, 0.7717601656913757, 0.7979999780654907], [0.8019999861717224, 0.8712980151176453, 0.7915058135986328, 0.8199999928474426], [0.7839999794960022, 0.860319972038269, 0.7817460298538208, 0.7879999876022339], [0.7950000166893005, 0.8559160232543945, 0.7944111824035645, 0.7960000038146973], [0.7919999957084656, 0.8719119429588318, 0.7979592084884644, 0.7820000052452087]]\n"
     ]
    }
   ],
   "source": [
    "model1_metrics = []\n",
    "model2_metrics = []\n",
    "for gen in test_generators:\n",
    "    model1_metrics.append(model1.evaluate(gen, verbose=0)[1:])\n",
    "    model2_metrics.append(model2.evaluate(gen, verbose=0)[1:])\n",
    "\n",
    "print(f'First model accuracies: {model1_metrics}')\n",
    "print(f'Second model accuracies: {model2_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_metrics = np.array(model1_metrics)\n",
    "model2_metrics = np.array(model2_metrics)\n",
    "\n",
    "model1_acc = model1_metrics[:, 0]\n",
    "model2_acc = model2_metrics[:, 0]\n",
    "\n",
    "model1_auc = model1_metrics[:, 1]\n",
    "model2_auc = model2_metrics[:, 1]\n",
    "\n",
    "model1_prec = model1_metrics[:, 2]\n",
    "model2_prec = model2_metrics[:, 2]\n",
    "\n",
    "model1_rec = model1_metrics[:, 3]\n",
    "model2_rec = model2_metrics[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 0.0\n",
      "Test pvalue: 3.814697265625e-06\n"
     ]
    }
   ],
   "source": [
    "stats, p = scipy.stats.wilcoxon(model1_acc, model2_acc)\n",
    "print(f\"Test statistic: {stats}\")\n",
    "print(f\"Test pvalue: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 0.0\n",
      "Test pvalue: 3.814697265625e-06\n"
     ]
    }
   ],
   "source": [
    "stats, p = scipy.stats.wilcoxon(model1_auc, model2_auc)\n",
    "print(f\"Test statistic: {stats}\")\n",
    "print(f\"Test pvalue: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 69.0\n",
      "Test pvalue: 0.312408447265625\n"
     ]
    }
   ],
   "source": [
    "stats, p = scipy.stats.wilcoxon(model1_prec, model2_prec)\n",
    "print(f\"Test statistic: {stats}\")\n",
    "print(f\"Test pvalue: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 0.0\n",
      "Test pvalue: 3.814697265625e-06\n"
     ]
    }
   ],
   "source": [
    "stats, p = scipy.stats.wilcoxon(model1_rec, model2_rec)\n",
    "print(f\"Test statistic: {stats}\")\n",
    "print(f\"Test pvalue: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a second question we can ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a significant difference between variances of two models?\n",
    "To answer this question we can use f-test. But there is one restriction: data cannot have normal distribution. To check, if data is normal we can perform *Shapiro-Wilk* test. Shapriro-Wilk test has a few steps. First we need to sort our test values ($x$) ascending. Then we have to compute: $SSE=\\sum_{i=1}^n(x_i-\\hat x)$. Then we need to calulate value $m$, based of the number of test cases ($n$): $m=\\frac{n}{2}$ if $n$ is even, $m=\\frac{n-1}{2}$ in the other case. After that we compute $b=\\sum_{i=1}^ma_i(x_{n+1-i}-x_i)$, $a_i$ we have to read from special table. Having value $b$ we can finally compute test statistics: $W=\\frac{b^2}{SSE}$. Fortunately we can perform this test using method *shapiro* from *scipy.stats*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model1 = load_model(models_dir+\"binary_model1.h5\")\n",
    "model2 = load_model(models_dir+\"binary_model2.h5\")\n",
    "\n",
    "model1.compile(optimizer=model1.optimizer,\n",
    "                        loss=model1.loss,\n",
    "                        metrics=['acc', 'auc', 'precision', 'recall'])\n",
    "model2.compile(optimizer=model2.optimizer,\n",
    "                        loss=model2.loss,\n",
    "                        metrics=['acc', 'auc', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tosiek/Documents/VI_semestr/GiGSN/statistical-significance-ml/.venv/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:890: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model accuracies: [[0.7699999809265137, 0.847681999206543, 0.8139534592628479, 0.699999988079071], [0.746999979019165, 0.8079699277877808, 0.7905882596969604, 0.671999990940094], [0.7400000095367432, 0.8189380168914795, 0.7898550629615784, 0.6539999842643738], [0.7490000128746033, 0.8129520416259766, 0.792941153049469, 0.6740000247955322], [0.7540000081062317, 0.8391139507293701, 0.8038277626037598, 0.671999990940094], [0.75, 0.8264439702033997, 0.7765486836433411, 0.7020000219345093], [0.7289999723434448, 0.8130620121955872, 0.7584649920463562, 0.671999990940094], [0.7390000224113464, 0.8294360637664795, 0.7697516679763794, 0.6819999814033508], [0.7630000114440918, 0.8403159976005554, 0.8036951422691345, 0.6959999799728394], [0.7540000081062317, 0.8262599110603333, 0.7834821343421936, 0.7020000219345093], [0.777999997138977, 0.8302860856056213, 0.8130630850791931, 0.722000002861023], [0.7139999866485596, 0.8054440021514893, 0.7661691308021545, 0.6159999966621399], [0.746999979019165, 0.8320460319519043, 0.7787810564041138, 0.6899999976158142], [0.7710000276565552, 0.838528037071228, 0.8044943809509277, 0.7160000205039978], [0.7289999723434448, 0.8126120567321777, 0.7632184028625488, 0.6639999747276306], [0.7630000114440918, 0.842456042766571, 0.8094117641448975, 0.6880000233650208], [0.7509999871253967, 0.8319480419158936, 0.7832956910133362, 0.6940000057220459], [0.7360000014305115, 0.8268299698829651, 0.7892156839370728, 0.6439999938011169], [0.7549999952316284, 0.8455559611320496, 0.8132678270339966, 0.6620000004768372]]\n",
      "Second model accuracies: [[0.781000018119812, 0.8760700225830078, 0.7707129120826721, 0.800000011920929], [0.7940000295639038, 0.8659219145774841, 0.7963709831237793, 0.7900000214576721], [0.7699999809265137, 0.8522260189056396, 0.7710843086242676, 0.7680000066757202], [0.7739999890327454, 0.8561200499534607, 0.7751004099845886, 0.7720000147819519], [0.8050000071525574, 0.8909260630607605, 0.8157349824905396, 0.7879999876022339], [0.7850000262260437, 0.8711420893669128, 0.7810651063919067, 0.7919999957084656], [0.7860000133514404, 0.8492399454116821, 0.7749999761581421, 0.8059999942779541], [0.7950000166893005, 0.8625040054321289, 0.7944111824035645, 0.7960000038146973], [0.8069999814033508, 0.8802520632743835, 0.803960382938385, 0.8119999766349792], [0.7730000019073486, 0.8644800186157227, 0.7640231847763062, 0.7900000214576721], [0.7950000166893005, 0.8696080446243286, 0.7842003703117371, 0.8140000104904175], [0.7680000066757202, 0.8468400239944458, 0.7887930870056152, 0.7319999933242798], [0.7730000019073486, 0.8657580018043518, 0.763005793094635, 0.7919999957084656], [0.7860000133514404, 0.8640180826187134, 0.7860000133514404, 0.7860000133514404], [0.781000018119812, 0.8658220171928406, 0.7717601656913757, 0.7979999780654907], [0.8019999861717224, 0.8712980151176453, 0.7915058135986328, 0.8199999928474426], [0.7839999794960022, 0.860319972038269, 0.7817460298538208, 0.7879999876022339], [0.7950000166893005, 0.8559160232543945, 0.7944111824035645, 0.7960000038146973], [0.7919999957084656, 0.8719119429588318, 0.7979592084884644, 0.7820000052452087]]\n"
     ]
    }
   ],
   "source": [
    "model1_metrics = []\n",
    "model2_metrics = []\n",
    "for gen in test_generators:\n",
    "    model1_metrics.append(model1.evaluate(gen, verbose=0)[1:])\n",
    "    model2_metrics.append(model2.evaluate(gen, verbose=0)[1:])\n",
    "\n",
    "print(f'First model accuracies: {model1_metrics}')\n",
    "print(f'Second model accuracies: {model2_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_metrics = np.array(model1_metrics)\n",
    "model2_metrics = np.array(model2_metrics)\n",
    "\n",
    "model1_acc = model1_metrics[:, 0]\n",
    "model2_acc = model2_metrics[:, 0]\n",
    "\n",
    "model1_auc = model1_metrics[:, 1]\n",
    "model2_auc = model2_metrics[:, 1]\n",
    "\n",
    "model1_prec = model1_metrics[:, 2]\n",
    "model2_prec = model2_metrics[:, 2]\n",
    "\n",
    "model1_rec = model1_metrics[:, 3]\n",
    "model2_rec = model2_metrics[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies of first model are not normally distrtibuted.\n",
      "Accuracies of second model are not normally distrtibuted.\n",
      "\n",
      "AUCes of first model are not normally distrtibuted.\n",
      "AUCes of second model are not normally distrtibuted.\n",
      "\n",
      "Precisions of first model are not normally distrtibuted.\n",
      "Precisions of second model are not normally distrtibuted.\n",
      "\n",
      "Recalls of first model are not normally distrtibuted.\n",
      "Recalls of second model are not normally distrtibuted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_normality(y, metric, model):\n",
    "    _, p = scipy.stats.shapiro(y)\n",
    "    if p < 0.001:\n",
    "        print(f'{metric} of {model} model are normally distrtibuted.')\n",
    "    else:\n",
    "        print(f'{metric} of {model} model are not normally distrtibuted.')\n",
    "\n",
    "# checking if accuracies are normally distributed\n",
    "check_normality(model1_acc, \"Accuracies\", \"first\")\n",
    "check_normality(model2_acc, \"Accuracies\", \"second\")\n",
    "print()\n",
    "# checking if auces are normally distributed\n",
    "check_normality(model1_auc, \"AUCes\", \"first\")\n",
    "check_normality(model2_auc, \"AUCes\", \"second\")\n",
    "print()\n",
    "# checking if precisions are normally distributed\n",
    "check_normality(model1_prec, \"Precisions\", \"first\")\n",
    "check_normality(model2_prec, \"Precisions\", \"second\")\n",
    "print()\n",
    "# checking if recalls are normally distributed\n",
    "check_normality(model1_rec, \"Recalls\", \"first\")\n",
    "check_normality(model2_rec, \"Recalls\", \"second\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case all of the metrics results are not normally distributed. Now, we can perform F-test. To calculate test statistic we use the equation below.\n",
    "$$F=\\frac{\\sigma_1^2}{\\sigma_2^2}$$\n",
    "This value follows f-distribution with $n-1$ and $n-1$ degrees of freedom, where $n$ is the number of test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20397118240415707"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1 = np.var(model1_acc)\n",
    "var2 = np.var(model2_acc)\n",
    "F = var1/var2\n",
    "n = len(model1_acc)\n",
    "p = 2*min(scipy.stats.f.sf(F, n-1, n-1), scipy.stats.f.cdf(F, n-1, n-1))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4545874378951895"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1 = np.var(model1_auc)\n",
    "var2 = np.var(model2_auc)\n",
    "F = var1/var2\n",
    "n = len(model1_auc)\n",
    "p = 2*min(scipy.stats.f.sf(F, n-1, n-1), scipy.stats.f.cdf(F, n-1, n-1))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3245173052057107"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1 = np.var(model1_prec)\n",
    "var2 = np.var(model2_prec)\n",
    "F = var1/var2\n",
    "n = len(model1_prec)\n",
    "p = 2*min(scipy.stats.f.sf(F, n-1, n-1), scipy.stats.f.cdf(F, n-1, n-1))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23245616713399564"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1 = np.var(model1_rec)\n",
    "var2 = np.var(model2_rec)\n",
    "F = var1/var2\n",
    "n = len(model1_rec)\n",
    "p = 2*min(scipy.stats.f.sf(F, n-1, n-1), scipy.stats.f.cdf(F, n-1, n-1))\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
